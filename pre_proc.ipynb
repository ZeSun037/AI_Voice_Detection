{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17397\\miniforge3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Essential module import\n",
    "\n",
    "import praatio\n",
    "from src.Charsiu import charsiu_chain_attention_aligner, charsiu_forced_aligner, charsiu_attention_aligner\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rainbow Passage 1_Speaker 1.wav', 'Rainbow Passage 2_Speaker 1.wav', 'Rainbow Passage 3_Speaker 1.wav', 'Rainbow Passage 4_Speaker 1.wav', 'Rainbow Passage 5_Speaker 1.wav']\n",
      "['Rainbow Passage 1.txt', 'Rainbow Passage 2.txt', 'Rainbow Passage 3.txt', 'Rainbow Passage 4.txt', 'Rainbow Passage 5.txt']\n"
     ]
    }
   ],
   "source": [
    "# Set up path to the data\n",
    "# We are expecting the following format of the files:\n",
    "\n",
    "# .WAV: [Passage Name] [Passage Corpus Number]_[Speaker] [Speaker Number].wav\n",
    "# .txt: [Passage Name] [Passage Corpus Number].txt\n",
    "path = \"../sample_data/original/\"\n",
    "audios = []\n",
    "transcripts = []\n",
    "for f in os.listdir(path):\n",
    "    if f.split('.')[1] == 'wav':\n",
    "        audios.append(f)\n",
    "    elif f.split('.')[1] == 'txt':\n",
    "        transcripts.append(f)\n",
    "\n",
    "print(audios)\n",
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17397\\miniforge3\\Lib\\site-packages\\transformers\\configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "charsiu = charsiu_attention_aligner('charsiu/en_w2v2_fs_10ms')\n",
    "fricatives = set(['F', 'Z', 'V', 'S'])\n",
    "alignments = []\n",
    "for voice, transcript in zip(audios, transcripts):\n",
    "    script = open(path + transcript).read()\n",
    "    alignment = charsiu.align(path + voice, script)\n",
    "    alignments.append(alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fricative_timestamps = []\n",
    "for alignment in alignments:\n",
    "    filtered = [f for f in alignment if f[-1] in fricatives]\n",
    "    fricative_timestamps.append(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system?       True\n",
      "CUDA version: 12.4\n",
      "ID of current CUDA device:       0\n",
      "Name of current CUDA device:       NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? \\\n",
    "      {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: \\\n",
    "      {torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device: \\\n",
    "      {torch.cuda.get_device_name(cuda_id)}\")\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alignment, audio in zip(fricative_timestamps, audios):\n",
    "\n",
    "    audio_name = audio.split('.')[0]\n",
    "\n",
    "    waveform, sample_rate = torchaudio.load(path + audio)\n",
    "    waveform = waveform.cpu().numpy()\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    target_dir = f\"../sample_data/segments/{audio_name}/\"\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "    \n",
    "    for ind, fricative in enumerate(alignment):\n",
    "\n",
    "        start, end = fricative[0], fricative[1]\n",
    "        phoneme = fricative[-1]\n",
    "\n",
    "        time_axis = torch.arange(start * sample_rate, end * sample_rate) / sample_rate\n",
    "        subwave = waveform[:, int(start * sample_rate):int(end * sample_rate)] \n",
    "        figure, axes = plt.subplots(num_channels, 1)\n",
    "\n",
    "        axes = [axes]\n",
    "        axes[0].plot(time_axis.cpu(), subwave[0], linewidth=1)\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        plt.ioff()        \n",
    "        figure.suptitle(f\"{audio_name}_{phoneme}_{ind}_Waveform\")\n",
    "        figure.savefig(target_dir + f\"{audio_name}_{phoneme}_{ind}_Waveform.png\")\n",
    "        plt.close(figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
